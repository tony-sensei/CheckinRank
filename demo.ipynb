{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Generate creds based on predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting gdown\r\n",
      "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: requests[socks] in ./venv/lib/python3.10/site-packages (from gdown) (2.28.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.10/site-packages (from gdown) (4.12.2)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from gdown) (4.65.0)\r\n",
      "Requirement already satisfied: six in ./venv/lib/python3.10/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from gdown) (3.12.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.12)\r\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\r\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\r\n",
      "Installing collected packages: PySocks, gdown\r\n",
      "Successfully installed PySocks-1.7.1 gdown-4.7.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch\n",
    "!pip install gdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "main_df = pd.read_csv(\"demo_datasets/main_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[\"time_label\"] = 0\n",
    "main_df[\"food_label\"] = 0\n",
    "main_df[\"serv_label\"] = 0\n",
    "main_df[\"env_label\"] = 0\n",
    "total_num = main_df.shape[0]\n",
    "total_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'I heard about Raising Cane\\'s via Louisiana friends who sang praises of the joint. While in NOLA (and surrounding suburbs) my companions and I spotted several RCs and after driving around for sometime we decided to stop in to see what all the fuss was about.\\n\\nMenu: reminiscent of California\\'s favorite burger joint, In-n-Out. The menu is basic and doesn\\'t offer many options. If my memory serves me correctly you have 3-4 combos to choose from. The combo is dependent upon the  # of chicken strips you want. A combo includes fries, drink, coleslaw, Texas toast and chicken strips.\\n\\nCustomer service: We were greeted when we walked in and promptly addressed when our food was ready. My one complaint: the cashier asked if I wanted lemonade or a fountain drink for my drink choice and I chose lemonade. I saw the lemonade behind the counter but didn\\'t think anything about it when she gave me my cup. I went to get my lemonade from the fountain and realized there wasn\\'t any lemonade. I decided to get sweet tea instead. However, after looking at my receipt I saw that I was charged an additional $0.30 for lemonade. I normally don\\'t make fusses at restaurants, but this time I decided to get what I paid for. When I asked the cashier if there was a charge for lemonade her quick reply was, \"Yeah, I charged you for it, but forgot to give you the lemonade.\" I don\\'t understand why she waited so long to fix her mistake. It\\'s just lemonade, but had I not checked my receipt or went back to claim my lemonade I would have missed out on the tasty lemonade from Raising Cane\\'s.\\n\\nDrinks: Lemonade: sweet, tart, and everything lemonade is supposed to be. Sweet tea: ok. Nothing to spectacular. McDonald\\'s and Chick-Fil-A offer better sweet tea.\\n\\nFood: Nothing to tweet or write home about. The fries were bland. Coleslaw was not bad! I usually skip coleslaw unless it\\'s from KFC, but RCs slaw was pretty tasty! Texas toast: warm, soft, and slightly buttery. I\\'ve never had Texas toast with sesame seeds, and most of my Texas toast tastings have been on the super buttery garlic side. The chicken strips were okay, but  tasted better with RCs special sauce. My companions and I tried to decipher the ingredients and we guessed ketchup, mayo, mustard, paprika, pepper, and salt. When it comes to fast food chicken and chicken strips, Chick-Fil-A has this delicacy on point. Raising Cane\\'s does a good job, but they can\\'t nab the gold.\\n\\nPrices: I paid $7.99 (including tax) for a massive drink, 4 chicken strips, toasted Texas toast, cole slaw, and fries. Can\\'t beat that price!\\n\\nOverall: Solid food, affordable prices, convenient. ||| You know the chicken fingers were fine nothing to special. The only reason why I\\'m writing a review is the dirty marketing behind Raising Canes, The Three Finger Combo! Walking through that lackluster mall, Raising Canes gave me a good chuckle and a beautiful picture. Eat here just for the humor. ||| As a California native, I can honestly say I wish we had a Cane\\'s. I like it better than Popeye\\'s and the price is great too. I ordered \"The Box\" combo with double-buttered toast and it totaled to less than $10. I wish I could have went here one more time before leaving NOLA. ||| The chicken fingers are pretty good. The seasoned fries are great. Just like any fried chicken places. ( KFC, POPEYE\\'S, etc..) ||| I was stuck at the mall with a hungry five year old, and she wanted chicken fingers. I can\\'t figure out why this chain is so popular, the chicken fingers don\\'t seem to be anything special to me, but my daughter put them away like there was no tomorrow. \\n\\nThe service was surprisingly friendly for a New Orleans fast food place. The \"Cane Sauce\" seems to be the only part of the meal that is above average. It has a nice spicy vinegar flavor, that goes well with the chicken and the french fries. Other than that, this place is thoroughly average. ||| I know its fast food but I love this place. I only get it once a year and I feel pretty unhealthy after I eat it but it\\'s totally worth it. Their sauce is my kryptonite! I always order several extra cups and save it for when we eat crawfish later on our trip. It\\'s perfect for dipping crawfish tails. There is something about that chicken, toast, crinkle cut fries and orange soda that just makes me smile (and my arteries clog). $7-8 on a caniac? Money well spent!'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[\"aggregated_text\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ReviewDatasetNoTarget(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "def create_main_data_loader_no_target(review_df, tokenizer, max_len, batch_size):\n",
    "    ds = ReviewDatasetNoTarget(\n",
    "        reviews= review_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "distilbert_model = DistilBertModel.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "distilbert_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.distilbert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.distilbert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    pooled_output = output.last_hidden_state[:, 0]\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "time_model = SentimentClassifier(3)\n",
    "food_model = SentimentClassifier(3)\n",
    "env_model = SentimentClassifier(3)\n",
    "serv_model = SentimentClassifier(3)\n",
    "\n",
    "# Take shareable link from google drive\n",
    "time_url = 'https://drive.google.com/file/d/1--RSVPrJ3nd_rRRysCDVrYoPTE_DMvQR/view?usp=sharing'\n",
    "time_destination = 'machine_learning/models/v3/best_model_wait_time.bin'\n",
    "gdown.download(time_url, time_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "food_url = 'https://drive.google.com/file/d/1-1NDuHhithta8QRAn8DUrAc9NjDwy-bZ/view?usp=sharing'\n",
    "food_destination = 'machine_learning/models/v3/best_model_food_quality.bin'\n",
    "gdown.download(food_url, food_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "env_url = 'https://drive.google.com/file/d/1-2tYpTLVKty2tCdiQ3FJzgrd54qwJ1_s/view?usp=sharing'\n",
    "env_destination = 'machine_learning/models/v3/best_model_environment_quality.bin'\n",
    "gdown.download(env_url, env_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "serv_url = 'https://drive.google.com/file/d/1-EIZTwVjfcnCiBqV55pGSXT-VfAPmozw/view?usp=sharing'\n",
    "serv_destination = 'machine_learning/models/v3/best_model_service_quality.bin'\n",
    "gdown.download(serv_url, serv_destination, quiet=False,fuzzy=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "time_model.load_state_dict(torch.load(time_destination, map_location=torch.device(device)))\n",
    "food_model.load_state_dict(torch.load(food_destination, map_location=torch.device(device)))\n",
    "env_model.load_state_dict(torch.load(env_destination, map_location=torch.device(device)))\n",
    "serv_model.load_state_dict(torch.load(serv_destination, map_location=torch.device(device)))\n",
    "\n",
    "time_model = time_model.to(device)\n",
    "food_model = food_model.to(device)\n",
    "env_model = env_model.to(device)\n",
    "serv_model = serv_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 512\n",
    "\n",
    "test_models = [time_model, food_model, env_model, serv_model]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing\n",
      "Stored in mainCred_demo.csv\n",
      "CPU times: user 9.61 s, sys: 11.1 s, total: 20.7 s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for main_idx, review_string in enumerate(main_df[\"aggregated_text\"]):\n",
    "  result_df = [] if review_string == '' else review_string.split('|||')\n",
    "\n",
    "  pred_data_loader = create_main_data_loader_no_target(\n",
    "      result_df, tokenizer, MAX_LEN, BATCH_SIZE\n",
    "      ) # change test_df to main\n",
    "  label_columns = ['time_label', 'food_label', 'env_label', 'serv_label']\n",
    "\n",
    "  result_df = pd.DataFrame(result_df, columns=['review'])\n",
    "  for idx, model in enumerate(test_models):\n",
    "      model_labels = []\n",
    "\n",
    "      for d in pred_data_loader:\n",
    "          input_ids = d[\"input_ids\"].to(device)\n",
    "          attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "              outputs = model(\n",
    "                  input_ids=input_ids,\n",
    "                  attention_mask=attention_mask\n",
    "              )\n",
    "          _, preds = torch.max(outputs, dim=1)\n",
    "          model_labels.extend(preds.cpu().numpy() - 1)\n",
    "\n",
    "      result_df[label_columns[idx]] = model_labels\n",
    "  for idx, model in enumerate(test_models):\n",
    "    main_df[label_columns[idx]][main_idx] = result_df[label_columns[idx]].mean()\n",
    "\n",
    "\n",
    "print(\"Finished processing\")\n",
    "main_df.to_csv('demo_datasets/mainCred_demo.csv', index=False)\n",
    "print(\"Stored in mainCred_demo.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
