{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate creds based on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch numpy pandas matplotlib gdown seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "main_df = pd.read_csv(\"demo_datasets/main_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[\"time_label\"] = 0\n",
    "main_df[\"food_label\"] = 0\n",
    "main_df[\"serv_label\"] = 0\n",
    "main_df[\"env_label\"] = 0\n",
    "total_num = main_df.shape[0]\n",
    "total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>aggregated_text</th>\n",
       "      <th>review_count</th>\n",
       "      <th>restaurant_count</th>\n",
       "      <th>income</th>\n",
       "      <th>sum_check_in</th>\n",
       "      <th>time_label</th>\n",
       "      <th>food_label</th>\n",
       "      <th>serv_label</th>\n",
       "      <th>env_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159651</td>\n",
       "      <td>Pretty sure this place doesn't exist. If you g...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>67100.0</td>\n",
       "      <td>35.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38826</td>\n",
       "      <td>Good place to go, awesome customer service and...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>96649.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37206</td>\n",
       "      <td>Man kept making saying words to me in Spanish ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>165089.0</td>\n",
       "      <td>182.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64002</td>\n",
       "      <td>The more unique items on the menu are the best...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88260.0</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61608</td>\n",
       "      <td>Great place to stop to grab a gyro or chicken ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>133874.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   population                                    aggregated_text   \n",
       "0      159651  Pretty sure this place doesn't exist. If you g...  \\\n",
       "1       38826  Good place to go, awesome customer service and...   \n",
       "2       37206  Man kept making saying words to me in Spanish ...   \n",
       "3       64002  The more unique items on the menu are the best...   \n",
       "4       61608  Great place to stop to grab a gyro or chicken ...   \n",
       "\n",
       "   review_count  restaurant_count    income  sum_check_in  time_label   \n",
       "0           5.0             266.0   67100.0         35.60           0  \\\n",
       "1           5.0              29.0   96649.0          1.24           0   \n",
       "2           5.0              17.0  165089.0        182.50           0   \n",
       "3           5.0              32.0   88260.0          3.94           0   \n",
       "4           5.0              61.0  133874.0          4.13           0   \n",
       "\n",
       "   food_label  serv_label  env_label  \n",
       "0           0           0          0  \n",
       "1           0           0          0  \n",
       "2           0           0          0  \n",
       "3           0           0          0  \n",
       "4           0           0          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDatasetNoTarget(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "def create_main_data_loader_no_target(review_df, tokenizer, max_len, batch_size):\n",
    "    ds = ReviewDatasetNoTarget(\n",
    "        reviews= review_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "distilbert_model = DistilBertModel.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "distilbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.distilbert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.distilbert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    pooled_output = output.last_hidden_state[:, 0]\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1--RSVPrJ3nd_rRRysCDVrYoPTE_DMvQR\n",
      "From (redirected): https://drive.google.com/uc?id=1--RSVPrJ3nd_rRRysCDVrYoPTE_DMvQR&confirm=t&uuid=09380b2a-50ae-4f2b-860c-ae419175172a\n",
      "To: /Users/tony/PycharmProjects/CheckinRank/machine_learning/models/v3/best_model_wait_time.bin\n",
      "100%|██████████| 266M/266M [00:19<00:00, 13.7MB/s] \n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1-1NDuHhithta8QRAn8DUrAc9NjDwy-bZ\n",
      "From (redirected): https://drive.google.com/uc?id=1-1NDuHhithta8QRAn8DUrAc9NjDwy-bZ&confirm=t&uuid=4f133c48-f2c6-46fb-a35a-633abd8e931a\n",
      "To: /Users/tony/PycharmProjects/CheckinRank/machine_learning/models/v3/best_model_food_quality.bin\n",
      "100%|██████████| 266M/266M [00:15<00:00, 17.1MB/s] \n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1-2tYpTLVKty2tCdiQ3FJzgrd54qwJ1_s\n",
      "From (redirected): https://drive.google.com/uc?id=1-2tYpTLVKty2tCdiQ3FJzgrd54qwJ1_s&confirm=t&uuid=858b042b-7a84-4386-9e30-22874a17e720\n",
      "To: /Users/tony/PycharmProjects/CheckinRank/machine_learning/models/v3/best_model_environment_quality.bin\n",
      "100%|██████████| 266M/266M [00:17<00:00, 14.9MB/s] \n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1-EIZTwVjfcnCiBqV55pGSXT-VfAPmozw\n",
      "From (redirected): https://drive.google.com/uc?id=1-EIZTwVjfcnCiBqV55pGSXT-VfAPmozw&confirm=t&uuid=b61765e2-d385-45fe-a89c-7452a6f18301\n",
      "To: /Users/tony/PycharmProjects/CheckinRank/machine_learning/models/v3/best_model_service_quality.bin\n",
      "100%|██████████| 266M/266M [00:17<00:00, 14.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'machine_learning/models/v3/best_model_service_quality.bin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "time_model = SentimentClassifier(3)\n",
    "food_model = SentimentClassifier(3)\n",
    "env_model = SentimentClassifier(3)\n",
    "serv_model = SentimentClassifier(3)\n",
    "\n",
    "# Take shareable link from google drive\n",
    "time_url = 'https://drive.google.com/file/d/1--RSVPrJ3nd_rRRysCDVrYoPTE_DMvQR/view?usp=sharing'\n",
    "time_destination = 'machine_learning/models/v3/best_model_wait_time.bin'\n",
    "gdown.download(time_url, time_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "food_url = 'https://drive.google.com/file/d/1-1NDuHhithta8QRAn8DUrAc9NjDwy-bZ/view?usp=sharing'\n",
    "food_destination = 'machine_learning/models/v3/best_model_food_quality.bin'\n",
    "gdown.download(food_url, food_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "env_url = 'https://drive.google.com/file/d/1-2tYpTLVKty2tCdiQ3FJzgrd54qwJ1_s/view?usp=sharing'\n",
    "env_destination = 'machine_learning/models/v3/best_model_environment_quality.bin'\n",
    "gdown.download(env_url, env_destination, quiet=False,fuzzy=True)\n",
    "\n",
    "serv_url = 'https://drive.google.com/file/d/1-EIZTwVjfcnCiBqV55pGSXT-VfAPmozw/view?usp=sharing'\n",
    "serv_destination = 'machine_learning/models/v3/best_model_service_quality.bin'\n",
    "gdown.download(serv_url, serv_destination, quiet=False,fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_model.load_state_dict(torch.load(time_destination, map_location=torch.device(device)))\n",
    "food_model.load_state_dict(torch.load(food_destination, map_location=torch.device(device)))\n",
    "env_model.load_state_dict(torch.load(env_destination, map_location=torch.device(device)))\n",
    "serv_model.load_state_dict(torch.load(serv_destination, map_location=torch.device(device)))\n",
    "\n",
    "time_model = time_model.to(device)\n",
    "food_model = food_model.to(device)\n",
    "env_model = env_model.to(device)\n",
    "serv_model = serv_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 512\n",
    "\n",
    "test_models = [time_model, food_model, env_model, serv_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for main_idx, review_string in enumerate(main_df[\"aggregated_text\"]):\n",
    "  result_df = [] if review_string == '' else review_string.split('|||')\n",
    "\n",
    "  pred_data_loader = create_main_data_loader_no_target(\n",
    "      result_df, tokenizer, MAX_LEN, BATCH_SIZE\n",
    "      ) # change test_df to main\n",
    "  label_columns = ['time_label', 'food_label', 'env_label', 'serv_label']\n",
    "\n",
    "  result_df = pd.DataFrame(result_df, columns=['review'])\n",
    "  for idx, model in enumerate(test_models):\n",
    "      model_labels = []\n",
    "\n",
    "      for d in pred_data_loader:\n",
    "          input_ids = d[\"input_ids\"].to(device)\n",
    "          attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "              outputs = model(\n",
    "                  input_ids=input_ids,\n",
    "                  attention_mask=attention_mask\n",
    "              )\n",
    "          _, preds = torch.max(outputs, dim=1)\n",
    "          model_labels.extend(preds.cpu().numpy() - 1)\n",
    "\n",
    "      result_df[label_columns[idx]] = model_labels\n",
    "  for idx, model in enumerate(test_models):\n",
    "    main_df[label_columns[idx]][main_idx] = result_df[label_columns[idx]].mean()\n",
    "\n",
    "\n",
    "print(\"Finished processing\")\n",
    "main_df.to_csv('demo_datasets/mainCred_demo.csv', index=False)\n",
    "print(\"Stored in mainCred_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = main_df.drop(columns=['aggregated_text']).corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
